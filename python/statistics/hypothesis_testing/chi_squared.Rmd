---
title: '<font size="5">Statistics in Python:</font><br>Chi-Squared Tests'
output:
    html_document:
        theme: paper
        highlight: textmate
        number_sections: true
        toc: true
        includes:
            before_body: ../../../google_analytics.html
---

<font size="3">

[⇦ Back](../../../python.html)

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(engine.path = "/usr/bin/python3.11")
```

A chi-squared (χ², pronounced "*kai-squared*") test is used for **discrete, categorical data**. For example:

- Medical data that includes two groups: people who are sick and people who are healthy
- Demographic data comparing people who are male and female
- Assessment data where people either passed or failed an assignment

Chi-squared tests are not relevant for continuous data, ie where each measurement/observation in the data set is a 'value' (eg a percentage, a height in cm, a weight in kg, etc).

```{r, echo=FALSE}
# install.packages("DiagrammeR", repos = "http://cran.us.r-project.org")
library(DiagrammeR)
DiagrammeR::grViz(
    "digraph statistical_tests {
        rankdir='LR'

        node [fontname=Helvetica, shape=box, width=5, fillcolor='#ea9999', style=filled, fontsize=42]
            'Start:\nSet a hypothesis'
        node [fontname=Helvetica, shape=box, width=5, fontsize=42, fillcolor=white, style=filled]
                'Continuous data'
                    'Relationships'
                        RIndependent [label='Independent\ngroups']
                            'Regression\nanalysis'
                                'Multiple\nlinear\nregression' [shape = 'oval']
                        RNonindependent [label='Non-independent\ngroups']
                            'Correlation\nanalysis'
                                RParametric [label='Parametric']
                                    'Pearson\nproduct-moment\nr correlation' [shape = 'oval']
                                RNonparametric [label='Non-parametric']
                                    'Spearman\nrank correlation' [shape = 'oval']
                    'Differences'
                        'Variances'
                        'Averages (means\nor medians)'
                            'One group'
                                OGParametric [label='Parametric']
                                    'One-sample t-test' [shape = 'oval']
                            'Two groups'
                                TGParametric [label='Parametric']
                                    PIndependent [label='Independent\ngroups']
                                        'Paired\ntwo-sample\nt-test' [shape = 'oval']
                                    PNonIndependent [label='Non-independent\ngroups']
                                        'Unpaired\ntwo-sample\nt-test' [shape = 'oval']
                                TGNonparametric [label='Non-parametric']
                                    TGIndependent [label='Independent\ngroups']
                                        'Mann-Whitney\nU test' [shape = 'oval']
                                    TGNonindependent [label='Non-independent\ngroups']
                                        'Wilcoxon\nsigned-rank test' [shape = 'oval']
                            'More than\ntwo groups'
                                MGParametric [label='Parametric']
                                    'ANOVA' [shape = 'oval']
                                MGNonparametric [label='Non-parametric']
                                    MGIndependent [label='Independent\ngroups']
                                        'Kruskal-Wallis\none-way ANOVA' [shape = 'oval']
                                    MGNonindependent [label='Non-independent\ngroups']
                                        'Friedman\ntwo-way ANOVA' [shape = 'oval']
        node [fontname=Helvetica, shape=box, width=5, fillcolor='#c9daf8', style=filled, fontsize=42]
                'Discrete,\ncategorical data'
                    'Chi-squared\ntest'
        node [fontname=Helvetica, shape=box, width=5, fillcolor='#d9ead3', style=filled, fontsize=42]
                        'Goodness-of-fit\ntest' [shape = 'oval']
                        'Independence\ntest' [shape = 'oval']
                        'Homogeneity\ntest' [shape = 'oval']
                        'Test for\ntrend' [shape = 'oval']

            'Start:\nSet a hypothesis' -> 'Continuous data'
                'Continuous data' -> 'Differences'
                    'Differences' -> 'Averages (means\nor medians)'
                        'Averages (means\nor medians)' -> 'One group'
                            'One group' -> OGParametric
                                OGParametric -> 'One-sample t-test'
                        'Averages (means\nor medians)' -> 'Two groups'
                            'Two groups' -> TGParametric
                                TGParametric -> PIndependent
                                    PIndependent -> 'Unpaired\ntwo-sample\nt-test'
                                TGParametric -> PNonIndependent
                                    PNonIndependent -> 'Paired\ntwo-sample\nt-test'
                            'Two groups' -> TGNonparametric
                                TGNonparametric -> TGIndependent
                                    TGIndependent -> 'Mann-Whitney\nU test'
                                TGNonparametric -> TGNonindependent
                                    TGNonindependent -> 'Wilcoxon\nsigned-rank test'
                        'Averages (means\nor medians)' -> 'More than\ntwo groups'
                            'More than\ntwo groups' -> MGParametric
                                MGParametric -> 'ANOVA'
                            'More than\ntwo groups' -> MGNonparametric
                                MGNonparametric -> MGIndependent
                                    MGIndependent -> 'Kruskal-Wallis\none-way ANOVA'
                                MGNonparametric -> MGNonindependent
                                    MGNonindependent -> 'Friedman\ntwo-way ANOVA'
                    'Differences' -> 'Variances'
                'Continuous data' -> 'Relationships'
                    'Relationships' -> RIndependent
                        RIndependent -> 'Regression\nanalysis'
                            'Regression\nanalysis' -> 'Multiple\nlinear\nregression'
                    'Relationships' -> RNonindependent
                        RNonindependent -> 'Correlation\nanalysis'
                            'Correlation\nanalysis' -> RParametric
                                RParametric -> 'Pearson\nproduct-moment\nr correlation'
                            'Correlation\nanalysis' -> RNonparametric
                                RNonparametric -> 'Spearman\nrank correlation'
            'Start:\nSet a hypothesis' -> 'Discrete,\ncategorical data'
                'Discrete,\ncategorical data' -> 'Chi-squared\ntest'
                    'Chi-squared\ntest' -> 'Goodness-of-fit\ntest'
                    'Chi-squared\ntest' -> 'Independence\ntest'
                    'Chi-squared\ntest' -> 'Homogeneity\ntest'
                    'Chi-squared\ntest' -> 'Test for\ntrend'

    labelloc='t';
    fontsize=70;
    label='Flowchart for Choosing a Statistical Test\n\n';
    }",
    width=900
)
```

Also note that for a chi-squared test to be valid *there must be more than 5 samples of each type*.

There are three main chi-squared tests:

- Goodness-of-fit
- Independence
- Homogeneity

In Python, all three are performed using the same function: `chisquare()` from the SciPy stats package. For more info about this function, see [its documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html).

A less-common chi-squared test is the "test for trend". This is not yet implemented in Python but worked examples are provided below.

Goodness-of-Fit Test
====================
This test answers the question __*does my distribution of numbers come from the distribution I expect*__?

Wikipedia Example
-----------------
The following example comes from [Wikipedia](https://en.wikipedia.org/wiki/Goodness_of_fit#Pearson's_chi-squared_test). If you randomly picked 100 people out of a population and 44 were male, is that population 50% male? You would have *observed* 44 males and 56 females, but your *expectation* would have been 50 males and 50 females. There are more than 5 samples in each group and the data is categorical (in this case it is *binary* - either male or female), as opposed to each data point being a measurement, so a chi-squared test can be considered. Take a look at how the information is entered into the `chisquare()` function in order to test the goodness-of-fit of your sample to the expected distribution:

```{python}
from scipy import stats

chisq, p = stats.chisquare([44, 56], [50, 50])

print(f'p = {p:4.2f}')
```

As you can see, the `chisquare()` statement takes the numbers that were *observed* in a list as the first argument and the numbers that were *expected* in a list as the second argument. The p-value of 0.23 suggests that there is a 23% chance that the population is 50% male/female.

As it happens, if you omit the second argument the `chisquare()` function assumes you expect equal numbers of each type of observation. Therefore, in this example (because we *expect* equal numbers of males and females) we can actually leave the second argument out:

```{python}
chisq, p = stats.chisquare([44, 56])

print(f'p = {p:4.2f}')
```

If we observed 50 males and 50 females, the test would tell use that there is a 100% chance that the population is 50% male/female:

```{python}
chisq, p = stats.chisquare([50, 50])

print(f'p = {p:4.2f}')
```

Note that the test does __NOT__ work properly if you give it your data as proportions or percentages:

```{python}
chisq, p = stats.chisquare([0.44, 0.56])

print(f'p = {p:4.2f}')
```

You must always provide the *actual numbers* observed and expected.

Crash Course Example
--------------------
The following example comes from [the Crash Course Youtube video](https://www.youtube.com/watch?v=7_cs1YlZoug):

|          | Healer | Tank | Assassin | Fighter |
|----------|--------|------|----------|---------|
| Observed | 25     | 35   | 50       | 90      |
| Expected | 30     | 40   | 40       | 90      |

Again, the data is categorical and each 'bin' (cell in the table) has more than 5 samples. So a chi-squared test is appropriate.

```{python}
chisq, p = stats.chisquare([25, 35, 50, 90], [30, 40, 40, 90])

print(f'p = {p:3.1f}')
```

Note that this example has 3 degrees of freedom. In general, the formula for degrees of freedom is `(nrows - 1) * (ncols - 1)`:

```{python}
nrows = 2
ncols = 4
df = (nrows - 1) * (ncols - 1)

print(f'Degrees of freedom = {df}')
```

This example can also be done manually by calculating the chi-square test statistic by hand:

$\chi^2 = \sum\frac{\left(observed - expected\right)^2}{expected}$

```{python}
chisq = (25 - 30)**2 / 30 + (35 - 40)**2 / 40 + (50 - 40)**2 / 40 + (90 - 90)**2 / 90

print(f'chi-square = {chisq:4.2f}')
```

Then the chi-square test statistic can be converted into a *p*-value using the chi-distribution for 3 degrees of freedom:

```{python}
p = 1 - stats.chi2.cdf(chisq, 3)

print(f'p = {p:3.1f}')
```

As the *p*-value is above 0.05 we *fail to reject the null hypothesis* that the distribution of characters chosen by players is as the developers claimed.

Independence Test
=================
The chi-squared independence test answers the question __*do these two (or more) samples come from the same distribution*__?

Crash Course Example
--------------------
Again using [an example from the Crash Course Youtube video](https://www.youtube.com/watch?v=7_cs1YlZoug):

**Observed:**

|     | Gryffindor | Hufflepuff | Ravenclaw | Slytherin |
|-----|------------|------------|-----------|-----------|
| No  | 79         | 122        | 204       | 74        |
| Yes | 82         | 139        | 240       | 69        |

**Expected:**

|     | Gryffindor | Hufflepuff | Ravenclaw | Slytherin |
|-----|------------|------------|-----------|-----------|
| No  | 77.12      | 120.71     | 212.68    | 68.5      |
| Yes | 83.88      | 131.29     | 231.32    | 74.5      |

In Python, this is calculated in the same way as the previous example except:

- An extra dimension is needed in each of the function's inputs
- The expected distribution is calculated from the row and column totals from the observed distribution
- The number of degrees of freedom is calculated instead of being hard-coded

```{python}
import numpy as np

# Are two distributions independent?
observations = np.array(
    [
        [79, 122, 204, 74],
        [82, 130, 240, 69]
    ]
)
row_totals = np.array([np.sum(observations, axis=1)])
col_totals = np.array([np.sum(observations, axis=0)])
n = np.sum(observations)
# Calculate the expected observations
expected = np.dot(row_totals.T, col_totals) / n
# Calculate the chi-square test statistic
chisq, p = stats.chisquare(observations, expected)
chisq = np.sum(chisq)
# Degrees of freedom
rows = observations.shape[0]
cols = observations.shape[1]
df = (rows - 1) * (cols - 1)
# Convert chi-square test statistic to p-value
p = 1 - stats.chi2.cdf(chisq, df)

print(f'p = {p:3.1f}')
```

As *p* is larger than 0.05 we *fail to reject the null hypothesis* that the distribution of Hogwarts houses is the same for both pineapple-on-pizza lovers and haters.

Stat Trek Example
-----------------
Using [an example from Stat Trek](https://stattrek.com/chi-square-test/independence.aspx):

**Observed:**

|        | Rep | Dem | Ind |
|--------|-----|-----|-----|
| Male   | 200 | 150 | 50  |
| Female | 250 | 300 | 50  |

```{python}
# Are two distributions independent?
observations = np.array(
    [
        [200, 150, 50],
        [250, 300, 50]
    ]
)
row_totals = np.array([np.sum(observations, axis=1)])
col_totals = np.array([np.sum(observations, axis=0)])
n = np.sum(observations)
# Calculate the expected observations
expected = np.dot(row_totals.T, col_totals) / n
# Calculate chi-square test statistic
chisq, p = stats.chisquare(observations, expected)
chisq = np.sum(chisq)
# Degrees of freedom
rows = observations.shape[0]
cols = observations.shape[1]
df = (rows - 1) * (cols - 1)
# Convert chi-square test statistic to p-value
p = 1 - stats.chi2.cdf(chisq, df)

print(f'p = {p:6.4f}')
```

To quote: "since the *p*-value (0.0003) is less than the significance level (0.05), we cannot accept the null hypothesis. Thus, we conclude that there is a relationship between gender and voting preference".

Homogeneity Test
================
The chi-squared homogeneity test answers the question __*do these two (or more) samples come from the same population*__?

The homogeneity test is that same as the independence test *in practice* (ie the way it is calculated in Python is the same) but it is different conceptually:

> The difference between a chi-squared independence test and a chi-squared homogeneity test is that, in the former, there is one group of people (ie one sample of the population) surveyed whereas in the second there are multiple groups surveyed.

As a result, the question you are asking about the equivalency of the sample distribution and the population distribution is different.

DisplayR Example
----------------
Using [an example from DisplayR](https://www.displayr.com/what-is-the-chi-square-test-of-homogeneity/):

**Observed:**

|                            | Living alone | Living with others |
|----------------------------|--------------|--------------------|
| On a diet                  | 2            | 25                 |
| Watch what I eat and drink | 23           | 146                |
| Whatever I feel like       | 7            | 124                |

```{python}
# Do different samples come from the same population?
observations = np.array(
    [
        [25, 146, 124],
        [2, 23, 7]
    ]
)
row_totals = np.array([np.sum(observations, axis=1)])
col_totals = np.array([np.sum(observations, axis=0)])
n = np.sum(observations)
# Calculate the expected observations
expected = np.dot(row_totals.T, col_totals) / n
# Calculate chi-square test statistic
chisq, p = stats.chisquare(observations, expected)
chisq = np.sum(chisq)
# Degrees of freedom
rows = observations.shape[0]
cols = observations.shape[1]
df = (rows - 1) * (cols - 1)
# Convert chi-square test statistic to p-value
p = 1 - stats.chi2.cdf(chisq, df)

print(f'p = {p:5.3f}')
```

To quote: "as this is greater than 0.05, by convention the conclusion is that the difference is due to sampling error, although the closeness of 0.05 to 0.052 makes this very much a 'line ball' conclusion."

Statsmodels Example
-------------------
Following on from [this page](one_sample_z_test_proportion.html) about the **one**-sample *Z*-test for a proportion and [this page](two_sample_z_test_proportion.html) about the **two**-sample *Z*-test for a proportion, it seems natural to want to do an example with **three** samples:

- $H_0: \pi_1 = \pi_2 = \pi_3$
- $H_1: \pi_1 \neq \pi_2$, $\pi_1 \neq \pi_3$ or $\pi_2 \neq \pi_3$

where $\pi$ indicates the true proportion of 'successes' for each cohort.

However, this is not yet possible with Statsmodels in Python:

```{python, eval = FALSE}
from statsmodels.stats.proportion import proportions_ztest

# Create example data: three cohorts of students take a test and these are the
# numbers that pass:
count = [39, 57, 54]
# While these are the total number in each cohort:
nobs = [60, 70, 65]

z_stat, p_value = proportions_ztest(count, nobs)
```

```
NotImplementedError: more than two samples are not implemented yet
```

*(Note also that Fisher's exact test for three (or more) groups is also currently not implemented in any Python package)*

However, as the *Z*-test for a proportion is mathematically equivalent to a chi-squared test, we can use `proportions_chisquare` from Statsmodels instead:

```{python}
from statsmodels.stats.proportion import proportions_chisquare

# Create example data: three cohorts of students take a test and these are the
# numbers that pass in each cohort:
count = [39, 57, 54]
# While these are the total number of students in each cohort:
nobs = [60, 70, 65]

# Three-Sample Chi-Squared Test for a Proportion
chi2_stat, p_value, table = proportions_chisquare(count, nobs)
```

Note that this takes `nobs` - the *total* number of observations - as the second input. This is different from SciPy's `chisquare` function which takes the numbers of observations in each *individual group* as its inputs.

As it outputs, `proportions_chisquare` gives us the tabulation of the number of observations:

```{python}
print(table[0])
```

...as well as the tabulation of expected observations:

```{python}
print(table[1])
```

...and the results:

```{python}
print(f'χ² = {chi2_stat:.2f}, p = {p_value:.3f}')
```

This low *p*-value can be understood by looking at the percentage of students who passed in each of the three cohorts:

```{python}
print(np.array(count) / np.array(nobs) * 100)
```

While the percentage of students achieving a pass mark was very similar for cohorts 2 and 3 (about 82%) the percentage was significantly lower for cohort 1 (65%). So, we shouldn't be surprised that the *p*-value suggests significance.

Test for Trend (Cochran–Armitage Test)
======================================
When a data set has a *nominal* input variable and an *ordinal* output variable, a chi-squared test for trend can be used[^1]. A nominal variable has no rank or order, for example 'gender' (male or female) or 'presence of illness' (healthy or diseased). An ordinal variable *does* have a rank or an order but the scale is arbitrary, such as 'level of satisfaction' (unsatisfied, neutral, satisfied) or 'dosage group' (low, medium, high). Both are *categorical* variables.

[^1]: Swinscow, TDV. Study design and choosing a statistical test. In: Campbell, MJ, editors. Statistics at Square One. BMJ Publishing Group; 1997. Available [here](https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/13-study-design-and-choosing-statisti). Jump to reference:&nbsp;

The chi-squared test for trend is also known as the "Cochran–Armitage test for trend" (see [here](https://en.wikipedia.org/wiki/Cochran%E2%80%93Armitage_test_for_trend) for more). It is not yet implemented in Python - neither in SciPy nor in Statsmodels - but it *is* implemented in R where it is called the `prop.trend.test` (see the documentation [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prop.trend.test)). What this means is that we can work through an example of this test in Python and then use the R function to check that what we have done is correct.

Influential Points Example
--------------------------
The first example we'll use comes from [Influential Points](https://influentialpoints.com/Training/chi_square_test_for_trend.htm). The raw data is given as a contingency table, but let's go a step backwards and generate this ourselves from a data set in *long format* to make it more realistic:

```{python}
# Generate raw data
x = []
y = []
x.extend(['Low'] * 5)
y.extend(['Infected'] * 5)
x.extend(['Medium'] * 14)
y.extend(['Infected'] * 14)
x.extend(['High'] * 18)
y.extend(['Infected'] * 18)
x.extend(['Low'] * 25)
y.extend(['Uninfected'] * 25)
x.extend(['Medium'] * 16)
y.extend(['Uninfected'] * 16)
x.extend(['High'] * 12)
y.extend(['Uninfected'] * 12)
```

We now have two lists which are each 90 elements long, corresponding to the 90 patients in our fictitious example. Now we can create the contingency table - also known as a cross-tabulation - using the `crosstab()` function in Pandas:

```{python}
import pandas as pd

# Create a contingency table from the raw data
ct = pd.crosstab(y, x)

print(ct)
```

The columns are not in the same order as on the Influential Points site and the table as a whole is not in the same orientation, but it doesn't matter.

The next thing to define is the scores, which are the arbitrary rankings (weights) which we will attach to each of the output variables ("High" = 3, "Medium" = 2, "Low" = 1). These need to be in the same order as the columns of the contingency table they refer to (High, Low, Medium):

```{python}
# Scores
score = [3, 1, 2]
```

Now we can perform the χ² test for trend. The formula used in the example has three intermediate variables: $T_1$, $T_2$ and $T_3$:

```{python}
import numpy as np

# T1
weighted_ct = ct * score
t_1 = weighted_ct.iloc[0, :].sum()
# T2
weighted_col_sums = weighted_ct.sum()
t_2 = weighted_col_sums.sum()
# T3
square_weighted_ct = ct * np.array(score)**2
square_weighted_col_sums = square_weighted_ct.sum()
t_3 = square_weighted_col_sums.sum()
```

We also need the overall sample size:

```{python}
# Sample size
col_sums = ct.sum(axis=1)
n = col_sums.sum()
```

These are all the elements we need to plug into their formula for χ²:

```{python}
# χ²
v = np.prod(col_sums) * (n * t_3 - t_2**2) / (n**2 * (n - 1))
chi_squared = (t_1 - (col_sums[0] * t_2 / n))**2 / v

print(chi_squared)
```

This matches their value of 11.51.

The last step is to test this value for significance by calculating the *p*-value by using the CDF (cumulative distribution function) for the χ² distribution:

```{python}
# Degrees of freedom
dof = np.prod(np.array(ct.shape) - 1) - 1
# p-value
# The probability of obtaining test results at least as extreme as the result
# actually observed, under the assumption that the null hypothesis is correct
p = 1 - stats.chi2.cdf(chi_squared, dof)

print(p)
```

This matches their result of 0.0007.

Using R we get the following:

```{r}
x  <- c(5, 14, 18)
n <- c(30, 30, 30)
prop.trend.test(x, n, c(1, 2, 3))
```

This is not 100% the same, but it's close enough.

Another Example
---------------
Let's take an imaginary example where students in various classes were asked if they were satisfied with the feedback they got for an assignment. We could hypothesise that there is an inverse trend between the amount of feedback a teaching assistant has the time to give and the size of the class, so let's look for this:

- $H_0:$ No true linear trend between the true proportion satisfied with assignment feedback and the size of class attended
- $H_1:$ True linear trend between the true proportion satisfied with assignment feedback and the size of class attended

```{python}
# Number of students satisfied with their level of feedback
count = [20, 15, 12, 8]
# Number of students who gave feedback
nobs = [30, 27, 23, 23]
# Relative size of class
weights = [1, 2, 3, 4]
```

Let's visualise the data:

```{python}
import matplotlib.pyplot as plt

percent_satisfied = np.array(count) / np.array(nobs)
plt.plot(weights, percent_satisfied)
plt.ylabel('Students Satisfied with their Feedback (%)')
plt.xlabel('Relative Class Size')
```

Now let's look for a trend:

```{python}
#
# Chi-squared test for trend
#
count_total = np.sum(count)
nobs_total = np.sum(nobs)
fails_total = nobs_total - count_total
T1 = np.sum(count * np.array(weights))
T2 = np.sum(nobs * np.array(weights))
T3 = np.sum(nobs * np.array(weights)**2)
V = count_total * fails_total * (nobs_total * T3 - T2**2) / (
    nobs_total**2 * (nobs_total - 1)
)
chi2_stat = (T1 - (count_total * T2 / nobs_total))**2 / V
p_value = 1 - stats.chi2.cdf(chi2_stat, df=1)

print(f'χ² = {chi2_stat:.3f}, p = {p_value:.3f}')
```

The chi-squared test for trend gives a *p*-value of less than 0.05, so we reject the null hypothesis in favour of the alternative. We conclude that there is evidence that the proportion of students satisfied with assignment feedback is truly related to the size of the class attended. As the size of the class attended increases, the proportion of satisfied students decreases.

This is consistent with the plot of the data which shows a downward trend. However, unlike linear regression, this hypothesis test does not provide an estimate of the constant rate of decrease of the proportion, i.e. an estimate of the gradient of the plot.

```{python, eval=FALSE, echo=FALSE}
# ANOTHER WORKED EXAMPLE WHICH, AS FAR AS I CAN TELL, IS INCORRECT IN THE SOURCE

"""
The chi-square test for trend.

Despina Koletsi, Nikolaos Pandis
STATISTICS AND RESEARCH DESIGN
VOLUME 150, ISSUE 6, P1066-1067, DECEMBER 01, 2016
DOI: https://doi.org/10.1016/j.ajodo.2016.10.001
https://www.ajodo.org/article/S0889-5406(16)30655-2/fulltext
"""
import pandas as pd
import numpy as np
from scipy import stats

x = []
y = []
x.extend(['Once'] * 16)
y.extend(['Yes'] * 16)
x.extend(['Twice'] * 9)
y.extend(['Yes'] * 9)
x.extend(['Seven Times'] * 2)
y.extend(['Yes'] * 2)
x.extend(['Once'] * 29)
y.extend(['No'] * 29)
x.extend(['Twice'] * 36)
y.extend(['No'] * 36)
x.extend(['Seven Times'] * 39)
y.extend(['No'] * 39)

ct = pd.crosstab(y, x)

# Scores
score = [1, 3, 2]

# Totals
col_sums = ct.sum(axis=0)
n = col_sums.sum()
row_sums = ct.sum(axis=1)

# Mean score for each category
weighted_ct = ct * score
weighted_row_sums = weighted_ct.sum(axis=1)
mean_categories = weighted_row_sums / row_sums
print(mean_categories)

# Mean score overall
weighted_col_sums = col_sums * score
weighted_total = weighted_col_sums.sum()
mean_overall = weighted_total / n
print(mean_overall)

# Sample standard deviation of the scores
weighted_ss = (score - mean_overall)**2 * col_sums
ddof = 1
std = np.sqrt(weighted_ss.sum() / (n - ddof))
print(std)

# χ²
chi_squared = mean_categories.sum()**2 / (std**2 * (1 / row_sums).sum())
print(chi_squared)

# *p*-value
# probability of obtaining test results at least as extreme as the result
# actually observed, under the assumption that the null hypothesis is correct
dof = np.prod(np.array(ct.shape) - 1)
p = 1 - stats.chi2.cdf(chi_squared, dof)
print(p)
```

A Function to Interpret *p*-values
==================================
Consider a new example:

> A company manufactures widgets. The mass of the widgets that are produced is controlled through batch testing: rather than weighing every single one they take a batch and weigh those instead. If the batch as a whole is too light or too heavy, then all of the widgets in that batch are sent for re-manufacture. The company's database shows that 90% of all batches are made within the acceptable mass range, while 5% are too heavy and 5% are too light.
>
> If the company were to install new manufacturing equipment they would want to know if they could expect this same distribution of acceptance and rejection. If they tested the first 1000 batches after installing the new equipment and 30 were too light and 60 were to heavy, would this be the case?

The problem above suggests that the observed numbers of too light, acceptable and too heavy batches was `[30, 910, 60]` whereas the expected numbers were `[50, 900, 50]`, respectively.

The null and alternative hypotheses are as follows:

- $H_0$: The new equipment follows the same distribution as the historical data
- $H_1$: The new equipment has a distribution different to the historical data

```{python}
def perform_chi_squared(obs, exp):
    """
    Perform a chi-squared test and interpret the p-value.

    The p-value is the probability of obtaining test results at least as
    extreme as the result actually observed, under the assumption that the null
    hypothesis is correct.
    """
    chisq, p = stats.chisquare(obs, exp)
    if p <= 0.001:
        significance = '***'
        interpretation = 'H_0 is rejected at a 99.9% confidence level.'
    elif p <= 0.01:
        significance = '**'
        interpretation = 'H_0 is rejected at a 99% confidence level.'
    elif p <= 0.05:
        significance = '*'
        interpretation = 'H_0 is rejected at a 95% confidence level.'
    else:
        significance = ''
        interpretation = 'H_0 is not rejected'
    return p, significance, interpretation


observations = [30, 910, 60]
expected = [50, 900, 50]
p, sig, inter = perform_chi_squared(observations, expected)

print(f'p = {p:5.3f}{sig}. {inter}')
```

Thus, at a 99% confidence level, we can say that the new equipment has a different distribution of overweight-underweight widget production compared to the old equipment.

[⇦ Back](../../../python.html)

</font>
